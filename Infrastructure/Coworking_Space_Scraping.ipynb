{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03eed1a6-eb90-43f0-8cef-01d90c0bbfe7",
   "metadata": {},
   "source": [
    "# Scraping Data from Top Coworking Space Websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef9e6237-961c-4df5-9c97-f63e5fa13bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "def scrape_innov8(city):\n",
    "    url=f'https://www.innov8.work/coworking-space/{city}/'\n",
    "    response = requests.get(url)\n",
    "    name=[]\n",
    "    location=[]\n",
    "    solutions=[]\n",
    "    price=[]\n",
    "    by=[]\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        spaces = soup.find_all('div', class_='ListingCard__without-map-container')\n",
    "        for space in spaces:\n",
    "            Name = space.find('h2', class_='property__name').text.strip()\n",
    "            name.append(Name)\n",
    "           \n",
    "            Location = space.find('div', class_='property__address').text.strip()\n",
    "            location.append(Location)\n",
    "            \n",
    "            Solutions =[solution.text for solution in space.find_all('div', class_='property__productName')]\n",
    "            for solution in Solutions:\n",
    "                solutions.append(solution)\n",
    "\n",
    "            Prices=[price.text.replace('₹','') for price in space.find_all('div', class_='property__productPrice') ]\n",
    "            for Price in Prices:\n",
    "                \n",
    "                if Price==\"NA\":\n",
    "                    price.append(\"NA\")\n",
    "                    by.append(\"NA\")\n",
    "                    continue\n",
    "                new_Price_list=Price.split('/')\n",
    "                price.append(new_Price_list[0])\n",
    "                by.append(new_Price_list[1])\n",
    "            for i in range(len(Prices)-1):\n",
    "                location.append(Location)\n",
    "                name.append(Name)\n",
    "\n",
    "        df = pd.DataFrame({'Name': name, 'Location': location, 'Solutions': solutions,'Price':price,'Per':by})\n",
    "          \n",
    "    else:\n",
    "        print(f'Failed to retrieve data from {url}')\n",
    "\n",
    "    return(df)\n",
    "\n",
    "def scrape_wework(city):\n",
    "    url=f'https://wework.co.in/{city}'\n",
    "    response = requests.get(url)\n",
    "    name=[]\n",
    "    location=[]\n",
    "    amenities=[]\n",
    "    Price=[]\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        spaces = soup.find_all('div', class_='MuiGrid-root MuiGrid-container MuiGrid-item MuiGrid-grid-xs-12 MuiGrid-grid-md-5.93 MuiGrid-grid-lg-5.93 card_container bau css-1exchbi')\n",
    "        for space in spaces:\n",
    "            Name = space.p.a.text.strip()\n",
    "            name.append(Name)\n",
    "           \n",
    "            \n",
    "            Location = space.find('p', class_='street').text.strip()\n",
    "            location.append(Location)\n",
    "            \n",
    "            Amenities =[amenity.text for amenity in space.find_all('p', class_='aminity_name')]\n",
    "            Amenities_str=','.join(Amenities)\n",
    "            amenities.append(Amenities_str)\n",
    "\n",
    "            Private_Office_url=space.find('a', class_='external_link')['href']\n",
    "            response2=requests.get(Private_Office_url)\n",
    "            soup2=BeautifulSoup(response2.content,'html.parser')\n",
    "            Private_Office_Cost=soup2.find('span', class_='price')\n",
    "            if Private_Office_Cost:\n",
    "                Private_Office_Cost_Sliced=Private_Office_Cost.text[slice(16,22)]\n",
    "                Price.append(Private_Office_Cost_Sliced.strip())\n",
    "            else:\n",
    "                Price.append('NA')\n",
    "        df = pd.DataFrame({'Name': name, 'Location': location, 'Amenities': amenities,'Price/Desk/Month':Price})\n",
    "          \n",
    "    else:\n",
    "        print(f'Failed to retrieve data from {url}')\n",
    "\n",
    "    return(df)\n",
    "\n",
    "            \n",
    "def scrape_91springboard(city):\n",
    "    url = f'https://www.91springboard.com/{city}/'\n",
    "    response = requests.get(url)\n",
    "    Locations=[]\n",
    "    connectivity=[]\n",
    "    solution_list=[]\n",
    "    Price=[]\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        spaces = soup.find_all('div', class_='col-12 col-md-4 py-3')\n",
    "        for space in spaces:\n",
    "            solutions=[]\n",
    "            location = space.find('h4', class_='card-title').text.strip()\n",
    "            Locations.append(location)\n",
    "            \n",
    "            Connectivity = space.find('p', class_='card-text f-16 mb-1').text.strip()\n",
    "            connectivity.append(Connectivity)\n",
    "\n",
    "            Solutions=[Solution.text.replace('₹', ':₹') for Solution in space.find('table', class_='table mb-1').find_all('tr')]\n",
    "            \n",
    "            for i in range(len(Solutions)-1):\n",
    "                Locations.append(location)\n",
    "                connectivity.append(Connectivity)\n",
    "            for Solution in Solutions:\n",
    "                price=Solution.split('₹')\n",
    "                solution_list.append(price[0])  \n",
    "                Price.append(price[1])\n",
    "            \n",
    "            Other_Amenities_url=space.a['href']\n",
    "            response2=requests.get(Other_Amenities_url)\n",
    "            soup2=BeautifulSoup(response2.content,'html.parser')\n",
    "            Other_Amenities=soup2.find_all('p', class_='pt-3')\n",
    "            #print(\"Other Amenities:\")\n",
    "            #for Others in Other_Amenities:\n",
    "                #print(Others.text)\n",
    "\n",
    "\n",
    "        df = pd.DataFrame({'Location': Locations,'Connectivity': connectivity,'Solutions': solution_list,'Price':Price})\n",
    "        \n",
    "    else:\n",
    "        print(f'Failed to retrieve data from {url}')\n",
    "    return(df)\n",
    "\n",
    "def scrape_regus(city):\n",
    "    Name=[]\n",
    "    Location=[]\n",
    "    Solution=[]\n",
    "    Price=[]\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(\"https://www.regus.com/en-gb\")\n",
    "    driver.implicitly_wait(10)\n",
    "    try:\n",
    "        driver.find_element(By.CLASS_NAME, value='ot-pc-refuse-all-handler').click()\n",
    "    except:\n",
    "        print(\"skipping\")\n",
    "    city_input=driver.find_element(By.ID, value='q')\n",
    "    city_input.send_keys(city)\n",
    "    time.sleep(2)\n",
    "    driver.find_element(By.XPATH,\"/html/body/div[1]/section[1]/div/div[2]/div[2]/div/div/div/form/div/div/div/div[2]/button\").click()\n",
    "    time.sleep(10)\n",
    "    driver.find_element(By.CLASS_NAME,value='rtl-1gd1xfc-loadMoreButton').click()\n",
    "    spaces=driver.find_elements(By.CLASS_NAME, value='rtl-1qtfy82-cardWrapper')\n",
    "    for space in spaces:\n",
    "        name=space.find_element(By.CLASS_NAME, value='rtl-yuhblz-cardTitle').get_attribute(\"innerText\").strip()\n",
    "        Name.append(name)\n",
    "        \n",
    "        location=space.find_element(By.CLASS_NAME, value='rtl-o9z7ux-cardAddress').get_attribute(\"innerText\").strip()\n",
    "        Location.append(location)\n",
    "        \n",
    "        solution=space.find_element(By.CLASS_NAME, value='rtl-4pm1ro-solutionItemName').get_attribute(\"innerText\").strip()\n",
    "        Solution.append(solution)\n",
    "        \n",
    "        price=space.find_element(By.CLASS_NAME, value='rtl-1v41wj9-solutionItemPrice').get_attribute(\"innerText\")\n",
    "        sliced_price=price[9:-4].strip()\n",
    "        if sliced_price=='l':\n",
    "            sliced_price='NA'\n",
    "        Price.append(sliced_price)\n",
    "    \n",
    "    df = pd.DataFrame({'Name': Name,'Location': Location,'Solutions': Solution,'Price':Price}) \n",
    "          \n",
    "    \n",
    "\n",
    "    return(df)\n",
    "\n",
    "\n",
    "def scrape_Awfis(city):\n",
    "    Name=[]\n",
    "    Name2=[]\n",
    "    Location=[]\n",
    "    Location2=[]\n",
    "    Price_main=[]\n",
    "    Amenities_main=[]\n",
    "    Amenities_main2=[]\n",
    "    Price_main2=[]\n",
    "    Seats=[]\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(\"https://www.awfis.com\")\n",
    "    \n",
    "    driver.implicitly_wait(10)\n",
    "    driver.find_element(By.ID, value='location').click()\n",
    "    driver.find_element(By.LINK_TEXT, f\"Coworking space in {city}\").click();\n",
    "    \n",
    "    names=driver.find_elements(By.CSS_SELECTOR, \"a.font-txt-white\")\n",
    "    names_cost=driver.find_elements(By.ID,\"6-seater\")\n",
    "    for name,nc in zip(names,names_cost):\n",
    "        new_cost=nc.find_element(By.TAG_NAME,\"h3\")\n",
    "        names_cost_stripped=new_cost.get_attribute(\"innerText\").replace('Rs.','').replace(',','').strip()\n",
    "        if int(names_cost_stripped)<3000:\n",
    "            continue\n",
    "        Name.append(name.get_attribute(\"innerText\"))\n",
    " \n",
    "\n",
    "    for i in range(len(Name)):\n",
    "        Price=[]\n",
    "        Amenities=[]\n",
    "        \n",
    "        Individual_Space=Name[i]\n",
    "        driver.find_element(By.LINK_TEXT, f\"{Individual_Space}\").click() \n",
    "        \n",
    "        Location_Parent=driver.find_element(By.CSS_SELECTOR, \"div.title-holder\")\n",
    "        location=Location_Parent.find_element(By.TAG_NAME,'h4')\n",
    "        Location.append(location.get_attribute(\"innerText\"))\n",
    "        \n",
    "        amenities=driver.find_elements(By.CSS_SELECTOR,\"a.font-14\") \n",
    "        \n",
    "        for amenity in amenities :\n",
    "            if amenity.get_attribute(\"innerText\") ==\"Cabins\" or amenity.get_attribute(\"innerText\")==\"Fixed Desks\":\n",
    "                Amenities.append(amenity.get_attribute(\"innerText\"))\n",
    "        for amenity in Amenities:\n",
    "            if amenity ==\"Cabins\" or amenity==\"Fixed Desks\":\n",
    "                current_amenity=driver.find_element(By.LINK_TEXT,f\"{amenity}\")\n",
    "                if len(Amenities)>1:\n",
    "                    current_amenity.click()\n",
    "                price=driver.find_element(By.CSS_SELECTOR,\"h3.price\").get_attribute(\"innerText\").replace('Rs.','')\n",
    "                Price.append(price)\n",
    "                try:\n",
    "                    seats=[seat.get_attribute(\"innerText\") for seat in driver.find_elements(By.CLASS_NAME,\"seater-click\")]\n",
    "                    Seats_str=','.join(seats)\n",
    "                    Seats.append(Seats_str)\n",
    "                except:\n",
    "                    Seats.append('NA')\n",
    "        time.sleep(10)\n",
    "        \n",
    "        if i==0:\n",
    "            driver.find_element(By.ID, value='zs-tip-close').click()\n",
    "            \n",
    "        driver.find_element(By.ID, value='location').click()\n",
    "        driver.find_element(By.LINK_TEXT, f\"Coworking space in {city}\").click();\n",
    "        Price_main.append(Price)\n",
    "        Amenities_main.append(Amenities)\n",
    "        \n",
    "        for j in range(len(Amenities)):\n",
    "            Name2.append(Name[i])\n",
    "            Location2.append(Location[i])\n",
    "            Price_main2.append(Price_main[i][j])\n",
    "            Amenities_main2.append(Amenities_main[i][j])\n",
    "\n",
    "    df = pd.DataFrame({'Name': Name2,'Location': Location2,'Solutions': Amenities_main2,'Price':Price_main2,'Seats' : Seats})  \n",
    "            \n",
    "    return(df)\n",
    "\n",
    "def scrape_91springboard_Bangalore_To_Excel(writer,df):\n",
    "    df.to_excel(writer,sheet_name='91Springboard Bangalore')\n",
    "    \n",
    "def scrape_wework_Bangalore_To_Excel(writer,df):\n",
    "    df.to_excel(writer,sheet_name='Wework Bangalore')\n",
    "\n",
    "def scrape_Awfis_Bangalore_To_Excel(writer,df):\n",
    "    df.to_excel(writer,sheet_name='Awfis Bangalore')\n",
    "    \n",
    "def scrape_91springboard_Hyderabad_To_Excel(writer,df):\n",
    "    df.to_excel(writer,sheet_name='91Springboard Hyderabad')\n",
    "    \n",
    "def scrape_wework_Hyderabad_To_Excel(writer,df):\n",
    "    df.to_excel(writer,sheet_name='Wework Hyderabad')\n",
    "\n",
    "def scrape_Awfis_Hyderabad_To_Excel(writer,df):\n",
    "    df.to_excel(writer,sheet_name='Awfis Hyderabad')\n",
    "\n",
    "def scrape_regus_Bangalore_To_Excel(writer,df):\n",
    "    df.to_excel(writer,sheet_name='Regus Bangalore')\n",
    "\n",
    "def scrape_innov8_Bangalore_To_Excel(writer,df)  :\n",
    "    df.to_excel(writer,sheet_name='innov8 Bangalore')\n",
    "\n",
    "def scrape_regus_Hyderabad_To_Excel(writer,df):\n",
    "    df.to_excel(writer,sheet_name='Regus Hyderabad')\n",
    "\n",
    "def scrape_innov8_Hyderabad_To_Excel(writer,df)  :\n",
    "    df.to_excel(writer,sheet_name='innov8 Hyderabad')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b84c783b-565d-4c8d-8f8c-2e72df8941a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=scrape_91springboard('bengaluru')\n",
    "df2=scrape_wework('bangalore')\n",
    "df3=scrape_91springboard('hyderabad')\n",
    "df4=scrape_wework('hyderabad')\n",
    "df7=scrape_Awfis('Bengaluru')\n",
    "df8=scrape_Awfis('Hyderabad')\n",
    "df9=scrape_regus('Bangalore')\n",
    "df10=scrape_regus(\"Hyderabad\")\n",
    "df11=scrape_innov8(\"Bangalore\")\n",
    "df12=scrape_innov8('Hyderabad')\n",
    "\n",
    "with pd.ExcelWriter('Cowork_Spaces.xlsx') as writer:\n",
    "    scrape_91springboard_Bangalore_To_Excel(writer,df1)\n",
    "    scrape_wework_Bangalore_To_Excel(writer,df2)\n",
    "    scrape_Awfis_Bangalore_To_Excel(writer,df7)\n",
    "    scrape_91springboard_Hyderabad_To_Excel(writer,df3)\n",
    "    scrape_Awfis_Hyderabad_To_Excel(writer,df8)\n",
    "    scrape_wework_Hyderabad_To_Excel(writer,df4)\n",
    "    scrape_regus_Bangalore_To_Excel(writer,df9)\n",
    "    scrape_innov8_Bangalore_To_Excel(writer,df11)  \n",
    "    scrape_regus_Hyderabad_To_Excel(writer,df10)\n",
    "    scrape_innov8_Hyderabad_To_Excel(writer,df12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f27c493-8ab3-4765-ba1a-d89ea909187a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
